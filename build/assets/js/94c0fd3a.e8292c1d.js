"use strict";(globalThis.webpackChunkqwen=globalThis.webpackChunkqwen||[]).push([[383],{8453(e,n,i){i.d(n,{R:()=>t,x:()=>c});var o=i(6540);const l={},s=o.createContext(l);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),o.createElement(s.Provider,{value:n},e.children)}},9739(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"modules/vla-module","title":"Vision-Language-Action (VLA) Module","description":"The Vision-Language-Action (VLA) module handles cognitive planning and multi-modal interaction, converting human instructions to robot actions.","source":"@site/docs/modules/vla-module.md","sourceDirName":"modules","slug":"/modules/vla-module","permalink":"/robot-system/docs/modules/vla-module","draft":false,"unlisted":false,"editUrl":"https://github.com/qwen/robot-system/tree/main/docs/docs/modules/vla-module.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AI-Robot Brain","permalink":"/robot-system/docs/modules/ai-robot-brain"}}');var l=i(4848),s=i(8453);const t={},c="Vision-Language-Action (VLA) Module",r={},a=[{value:"Overview",id:"overview",level:2},{value:"Components",id:"components",level:2},{value:"Voice Processing",id:"voice-processing",level:3},{value:"Speech Recognition",id:"speech-recognition",level:4},{value:"Text-to-Speech",id:"text-to-speech",level:4},{value:"NLP Processor",id:"nlp-processor",level:3},{value:"Intent Recognition",id:"intent-recognition",level:4},{value:"Entity Extraction",id:"entity-extraction",level:4},{value:"Action Planner",id:"action-planner",level:3},{value:"Plan Generation",id:"plan-generation",level:4},{value:"Plan Execution",id:"plan-execution",level:4},{value:"Launch Files",id:"launch-files",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Topics and Services",id:"topics-and-services",level:2},{value:"Published Topics",id:"published-topics",level:3},{value:"Subscribed Topics",id:"subscribed-topics",level:3},{value:"Services",id:"services",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:2},{value:"Supported Commands",id:"supported-commands",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"vision-language-action-vla-module",children:"Vision-Language-Action (VLA) Module"})}),"\n",(0,l.jsx)(n.p,{children:"The Vision-Language-Action (VLA) module handles cognitive planning and multi-modal interaction, converting human instructions to robot actions."}),"\n",(0,l.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,l.jsx)(n.p,{children:"The VLA Module is responsible for:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Voice command processing and natural language understanding"}),"\n",(0,l.jsx)(n.li,{children:"Multi-modal interaction (vision, language, action)"}),"\n",(0,l.jsx)(n.li,{children:"Cognitive planning and reasoning"}),"\n",(0,l.jsx)(n.li,{children:"Converting high-level commands to executable actions"}),"\n",(0,l.jsx)(n.li,{children:"Human-robot interaction"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"components",children:"Components"}),"\n",(0,l.jsx)(n.h3,{id:"voice-processing",children:"Voice Processing"}),"\n",(0,l.jsx)(n.p,{children:"The voice processing component handles speech recognition and synthesis:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"class VoiceService(Node):\n    def __init__(self):\n        super().__init__('voice_service')\n        # Speech recognition, text-to-speech, audio processing\n"})}),"\n",(0,l.jsx)(n.h4,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Real-time speech-to-text conversion"}),"\n",(0,l.jsx)(n.li,{children:"Noise cancellation"}),"\n",(0,l.jsx)(n.li,{children:"Speaker identification"}),"\n",(0,l.jsx)(n.li,{children:"Language model integration"}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"text-to-speech",children:"Text-to-Speech"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Natural voice synthesis"}),"\n",(0,l.jsx)(n.li,{children:"Emotional tone control"}),"\n",(0,l.jsx)(n.li,{children:"Multi-language support"}),"\n",(0,l.jsx)(n.li,{children:"Audio output management"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"nlp-processor",children:"NLP Processor"}),"\n",(0,l.jsx)(n.p,{children:"The Natural Language Processing component interprets user commands:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"class NlpProcessor(Node):\n    def __init__(self):\n        super().__init__('nlp_processor')\n        # Intent recognition, entity extraction, command parsing\n"})}),"\n",(0,l.jsx)(n.h4,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Classification of user intent (navigate, manipulate, report status)"}),"\n",(0,l.jsx)(n.li,{children:"Context-aware understanding"}),"\n",(0,l.jsx)(n.li,{children:"Command validation"}),"\n",(0,l.jsx)(n.li,{children:"Error handling"}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"entity-extraction",children:"Entity Extraction"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Location identification"}),"\n",(0,l.jsx)(n.li,{children:"Object recognition in text"}),"\n",(0,l.jsx)(n.li,{children:"Action parameter extraction"}),"\n",(0,l.jsx)(n.li,{children:"Reference resolution"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"action-planner",children:"Action Planner"}),"\n",(0,l.jsx)(n.p,{children:"The action planning component converts high-level commands into executable action sequences:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"class ActionPlanner(Node):\n    def __init__(self):\n        super().__init__('action_planner')\n        # Action sequence generation, plan optimization\n"})}),"\n",(0,l.jsx)(n.h4,{id:"plan-generation",children:"Plan Generation"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"High-level command interpretation"}),"\n",(0,l.jsx)(n.li,{children:"Action sequence creation"}),"\n",(0,l.jsx)(n.li,{children:"Resource allocation"}),"\n",(0,l.jsx)(n.li,{children:"Plan validation"}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"plan-execution",children:"Plan Execution"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Action sequencing"}),"\n",(0,l.jsx)(n.li,{children:"Execution monitoring"}),"\n",(0,l.jsx)(n.li,{children:"Plan adjustment"}),"\n",(0,l.jsx)(n.li,{children:"Failure recovery"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"launch-files",children:"Launch Files"}),"\n",(0,l.jsx)(n.p,{children:"The VLA Module can be launched using:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ros2 launch vla_module vla_system.launch.py\n"})}),"\n",(0,l.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,l.jsx)(n.p,{children:"The VLA Module can be configured through:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"config/voice.yaml"})," - Voice processing parameters"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"config/nlp.yaml"})," - NLP model and parameters"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"config/planning.yaml"})," - Action planning parameters"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"config/models/"})," - Language and vision model configurations"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"topics-and-services",children:"Topics and Services"}),"\n",(0,l.jsx)(n.h3,{id:"published-topics",children:"Published Topics"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/processed_user_command"})," - Commands with intent and parameters"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/action_plan"})," - Generated action sequences"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/robot_feedback"})," - Feedback to the user"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/voice_output"})," - Text to be spoken"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"subscribed-topics",children:"Subscribed Topics"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/user_audio"})," - Raw audio input"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/robot_state"})," - Current robot state"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/perception_results"})," - Environmental perception data"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/navigation_path"})," - Navigation information"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"services",children:"Services"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/process_command"})," - Service to process a text command"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/generate_action_plan"})," - Service to create an action plan"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"/speak_text"})," - Service to have robot speak text"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,l.jsx)(n.p,{children:"The VLA Module integrates multiple modalities:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Vision: Uses perception data to understand the environment"}),"\n",(0,l.jsx)(n.li,{children:"Language: Processes natural language commands"}),"\n",(0,l.jsx)(n.li,{children:"Action: Generates executable action sequences"}),"\n",(0,l.jsx)(n.li,{children:"Context: Maintains conversation and task context"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"supported-commands",children:"Supported Commands"}),"\n",(0,l.jsx)(n.p,{children:"The system understands various types of commands:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:'Navigation: "Go to the kitchen", "Move to the living room"'}),"\n",(0,l.jsx)(n.li,{children:'Manipulation: "Pick up the red ball", "Put the book on the table"'}),"\n",(0,l.jsx)(n.li,{children:'Status: "Report your status", "What can you see?"'}),"\n",(0,l.jsx)(n.li,{children:'Complex tasks: "Go to the kitchen and bring me the water bottle"'}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Efficient NLP model usage"}),"\n",(0,l.jsx)(n.li,{children:"Context-aware processing"}),"\n",(0,l.jsx)(n.li,{children:"Multi-threaded execution"}),"\n",(0,l.jsx)(n.li,{children:"Caching of common operations"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"If voice recognition fails, check audio input quality"}),"\n",(0,l.jsx)(n.li,{children:"If commands are misunderstood, verify NLP model"}),"\n",(0,l.jsx)(n.li,{children:"If actions aren't planned correctly, check action planner"}),"\n",(0,l.jsx)(n.li,{children:"If responses are slow, optimize model inference"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}}}]);